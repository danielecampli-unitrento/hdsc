{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59aef9d9",
   "metadata": {},
   "source": [
    "# HDSC: knowledge base embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012a3d63",
   "metadata": {},
   "source": [
    "Let's import all the libraries needed for our knowledge base embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a84fb6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import warnings\n",
    "import torchhd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from urllib3.exceptions import NotOpenSSLWarning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0830f90",
   "metadata": {},
   "source": [
    "Now, let's configure the runtime environment settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "286d5502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I/O configuration\n",
    "input_folder = '../data/ncdf'\n",
    "output_folder = '../data/ncdf'\n",
    "\n",
    "# Runtime configuration\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Ignore annoying urllib3 warnings\n",
    "warnings.filterwarnings(\"ignore\", category=NotOpenSSLWarning)\n",
    "\n",
    "# Load dataset\n",
    "dataset = xr.load_dataarray(os.path.join(input_folder, \"nrm.orx.dataset.nc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ffd7c1",
   "metadata": {},
   "source": [
    "Our refine dataset can be used in two ways: 1. as-is, or 2. upon hyperdimensional encoding. The as-is solution, since the signals are more or less orthogonal, should work, but if there is not enough randomness across the components of our vectors, the crosstalk/noise when querying the classifying dictionary can significantly degrade the overall accuracy of the classifier. Let's see if this phenomenon happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52cc02fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HRRTensor([[-0.9791,  0.7290, -0.1287,  ..., -0.8407,  0.9690,  0.6006]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<StemContainer object of 3 artists>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "testset_1mhz = torchhd.HRRTensor(dataset.sel(frequency=1).values)\n",
    "radius_embeddings = torchhd.HRRTensor(torch.randn(45, dataset.time.size))\n",
    "\n",
    "# Check if the shape of our hypervectors are the same\n",
    "# print(testset_1mhz.shape, radius_embeddings.shape)\n",
    "\n",
    "kbe = torchhd.hash_table(testset_1mhz, radius_embeddings)\n",
    "\n",
    "# Let's assume we want to retrieve the radius size for the first signal in our testset@1MHz\n",
    "query = torchhd.inverse(testset_1mhz[0])\n",
    "result = torchhd.bind(kbe, query)\n",
    "\n",
    "# Now let's perform a memory cleanup: the result vector should be the closest to our simulated signal at frequency=1MHz\n",
    "print(torchhd.cleanup(result, radius_embeddings))\n",
    "\n",
    "torchhd.utils.plot_similarity(result, radius_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f1e847",
   "metadata": {},
   "source": [
    "Just running a few simulation of the previous cell, we can see through the plot that the element in position 0 (i.e. radius = 10) has stronger similarity to our dictionary classification around 1 out of 10 trials. Naturally, this is not acceptable; we then have to encode our signals in a way that allows to 1. preserve the signal information geometry and 2. provide accurate classifications. The torchhd framework provides several encoding methods. The function **hdencode()** will wrap some of them, also providing a more comfortable interface to encode our signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5b78c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hdencode(data, in_features, out_features, embed_type=\"random\", vsa=\"BSC\"):\n",
    "    mapping = {\n",
    "        \"random\": torchhd.embeddings.Random,\n",
    "        \"level\": torchhd.embeddings.Level,\n",
    "        \"density\": torchhd.embeddings.Density,\n",
    "        \"thermometer\": torchhd.embeddings.Thermometer,\n",
    "        \"projection\": torchhd.embeddings.Projection,        # MAP, HRR, VTB\n",
    "        \"sinusoid\": torchhd.embeddings.Sinusoid,            # MAP, HRR, VTB\n",
    "        \"fractional\": torchhd.embeddings.FractionalPower    # HRR, FHRR\n",
    "    }\n",
    "    if embed_type not in mapping:\n",
    "        raise ValueError(f\"Unknown embedding type: {embed_type}\")\n",
    "    embedding = mapping[embed_type](in_features, out_features, vsa=vsa)\n",
    "    return embedding(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5b64b7",
   "metadata": {},
   "source": [
    "We can verify to what extent the torchhd encoding methods are similarity-preserving or not. The cell below executes a simulation averaging the cosine similarity between pairs $(x, y)$ and $(\\Gamma(x), \\Gamma(y))$, where x and y are really similar, and $\\Gamma(x)$ and $\\Gamma(y)$ should be similar if the embedding $\\Gamma$ is similarity-preserving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbc5545b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HDC encoding: 100%|██████████| 100/100 [00:32<00:00,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity levels for encoding method:\n",
      "fractional :: 0.846644\n",
      "projection :: 0.999983\n",
      "sinusoid   :: 0.999962\n",
      "density    :: 0.996591\n",
      "baseline   :: 0.999983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n = 10000\n",
    "no_simulations = 100\n",
    "\n",
    "# Circular, thermometer embeddings are computationally very expensive and prone to\n",
    "# memory overflow and crashes.\n",
    "\n",
    "fractional = torchhd.embeddings.FractionalPower(n, n, \"gaussian\", 1.0, \"FHRR\")\n",
    "projection = torchhd.embeddings.Projection(10000, 10000)\n",
    "sinusoid = torchhd.embeddings.Sinusoid(10000, 10000, vsa=\"HRR\")\n",
    "density = torchhd.embeddings.Density(n, n, vsa=\"FHRR\")\n",
    "\n",
    "hdembed = {\n",
    "    \"fractional\": fractional,\n",
    "    \"projection\": projection,\n",
    "    \"sinusoid\": sinusoid,\n",
    "    \"density\": density\n",
    "}\n",
    "\n",
    "avg_cos_sim = {name: 0.0 for name in hdembed.keys()}\n",
    "avg_cos_sim[\"baseline\"] = 0.0\n",
    "\n",
    "for _ in tqdm(range(no_simulations), desc=\"HDC encoding\"):\n",
    "    x = torch.randn(1, 10000)\n",
    "    epsilon = 0.01\n",
    "    noise = (torch.rand_like(x) - 0.5) * 2 * epsilon  # uniform in [-epsilon, +epsilon]\n",
    "    y = x + noise\n",
    "    cos_sim_orig = torchhd.cosine_similarity(x, y).item()\n",
    "    avg_cos_sim[\"baseline\"] += cos_sim_orig\n",
    "    for name, emb in hdembed.items():\n",
    "        x_hd, y_hd = hdembed[name](x), hdembed[name](y)\n",
    "        cos_sim = torchhd.cosine_similarity(x_hd, y_hd).item()\n",
    "        avg_cos_sim[name] += cos_sim\n",
    "\n",
    "for name in avg_cos_sim:\n",
    "    avg_cos_sim[name] /= no_simulations\n",
    "\n",
    "print(\"Cosine similarity levels for encoding method:\")\n",
    "for name, sim in avg_cos_sim.items():\n",
    "    print(f\"{name:<10} :: {sim:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e516c2",
   "metadata": {},
   "source": [
    "Let's test our **hdencode()** method starting from some random values; we can use this technique to create labeling hypervectors for both central frequencies and radii size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b749f948",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "no_radii = dataset.radius.size\n",
    "no_frequencies = dataset.frequency.size\n",
    "embedding_size = 1024\n",
    "random_embedding_size = 1024\n",
    "vsa_encoding = \"BSC\"\n",
    "\n",
    "frequency_embeddings = hdencode(\n",
    "    torch.randn(no_frequencies, random_embedding_size),\n",
    "    in_features=random_embedding_size,\n",
    "    out_features=embedding_size,\n",
    "    embed_type=\"density\",\n",
    "    vsa=vsa_encoding)\n",
    "\n",
    "radius_embeddings = hdencode(\n",
    "    torch.randn(no_radii, random_embedding_size),\n",
    "    in_features=random_embedding_size,\n",
    "    out_features=embedding_size,\n",
    "    embed_type=\"density\",\n",
    "    vsa=vsa_encoding)\n",
    "\n",
    "print(f\"Frequencies embeddings: {np.shape(frequency_embeddings)}\")\n",
    "print(f\"Radii embeddings: {np.shape(radius_embeddings)}\")\n",
    "print(frequency_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfca08dd",
   "metadata": {},
   "source": [
    "Since we have **hdencode()** at our disposal, we can use it to test its capabitilies. We will take the signals generated by *all* the central frequencies resonating on an acoustic trap of radius=300 (micrometers), and use them to create a classifying dictionary. Let's encode our signals using a *density*-based sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d40944",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_embeddings = torch.tensor(dataset.sel(radius=300).values)\n",
    "signal_embeddings = xr.apply_ufunc(\n",
    "    hdencode,\n",
    "    testset_embeddings,\n",
    "    dataset.time.size,\n",
    "    embedding_size,\n",
    "    \"density\",\n",
    "    vsa_encoding,\n",
    "    vectorize=True)\n",
    "print(f\"Signals embeddings: {np.shape(signal_embeddings)}\")\n",
    "print(signal_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e5ea92",
   "metadata": {},
   "source": [
    "At this point, we have 10 signal embeddings (all frequencies, radius=300), and 10 central frequencies embeddings. We can use the latter as keys, while the former as values of our dictionary. From now on, we will refer to this kind of dictionaries to **kbe**s -knowledge base embeddings-, since their role is to preserve the categorical structure of our dataset. Let's create our kbe, and then we will test it using as query the 1MHz frequency embedding (i.e., frequency_embedding[0]). A dictionary lookup using such frequency embedding as key yields a result vector, and this vector should be the closest to the simulated signal parametrized by radius=300 and frequency=1Mhz. To confirm that our dictionary works as expected, the plot should show the highest bar in position 0.\n",
    "\n",
    "IMPORTANT NOTE: using central frequecies embeddings as keys and signals embeddings as values is a completely arbitrary choice. The results do not change if we invert their role, i.e., signals embeddings as keys, and frequencies embeddings as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11264a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the kbe (knowledge base embedding) for radius=300\n",
    "# Operations syntax:                   bind := * | bundle := +\n",
    "# kbe[radius=300] :=\n",
    "#   [frequency=1MHz] * [signal=(radius=10,frequency=1MHz)    +\n",
    "#   [frequency=1MHz] * [signal=(radius=10,frequency=2MHz)    +\n",
    "#   ...\n",
    "#   [frequency=10MHz] * [signal=(radius=10,frequency=9MHz)]  +\n",
    "#   [frequency=10MHz] * [signal=(radius=10,frequency=10MHz)]\n",
    "\n",
    "kbe = torchhd.hash_table(frequency_embeddings, signal_embeddings)\n",
    "\n",
    "# Let's assume we want to know what is the signal like at 1MHz\n",
    "query_embedding = torchhd.inverse(frequency_embeddings[0])\n",
    "result = torchhd.bind(kbe, query_embedding)\n",
    "# Now let's perform a memory cleanup: the result vector should be the\n",
    "# closest to our simulated signal at frequency=4MHz\n",
    "print(torchhd.cleanup(result, signal_embeddings))\n",
    "\n",
    "torchhd.utils.plot_similarity(result, signal_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525c18f9",
   "metadata": {},
   "source": [
    "We can see that the result has highest similarity with element in position 0, which corresponds to the embedding of the signal having central frequency equal to 1MHz. We can do a number of simulations to test the accuracy of such classification, and see if our results vary from a dictionary created starting from completely random hypervectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98088b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_simulations = 100\n",
    "correct_validations = 0\n",
    "embedding_size = 1024\n",
    "random_embedding_size = 1024\n",
    "vsa_encoding = \"BSC\"\n",
    "\n",
    "no_simulations = 10\n",
    "correct_validations = 0\n",
    "for i in np.arange(no_simulations):\n",
    "    frequency_embeddings = hdencode(\n",
    "        torch.randn(no_frequencies, random_embedding_size),\n",
    "        in_features=random_embedding_size,\n",
    "        out_features=embedding_size,\n",
    "        embed_type=\"density\",\n",
    "        vsa=vsa_encoding)\n",
    "    query_index = 0   # Our query will be random_keys_embeddings[0]\n",
    "    query_embedding = frequency_embeddings[query_index]\n",
    "    correct_result = signal_embeddings[query_index]\n",
    "    kbe = torchhd.hash_table(frequency_embeddings, signal_embeddings)\n",
    "    # Let's assume we want to know what is the signal like at 4MHz\n",
    "    query = torchhd.inverse(query_embedding)\n",
    "    result = torchhd.bind(kbe, query)\n",
    "    nearest_match = torchhd.cleanup(result, signal_embeddings)\n",
    "    # nearest_match = nearest_inner_prod(result, frequency_embeddings)\n",
    "    if (nearest_match == correct_result).all():\n",
    "        correct_validations += 1\n",
    "print(f\"Query matching accuracy: {correct_validations / no_simulations * 100}%\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3c29b5",
   "metadata": {},
   "source": [
    "Running the subsequent cell, we can observe how we can create symbolic dictionaries starting from completely random hypervectors. This symbolic aspect is one the main advantages of hyperdimensional computing/vector symbolic architectures (HDC/VSA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04612bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_keys, no_values = 10, 10\n",
    "embedding_size = 1024\n",
    "random_embedding_size = 1024\n",
    "vsa_encoding = \"BSC\"\n",
    "\n",
    "random_keys_embeddings = hdencode(\n",
    "    torch.randn(no_keys, random_embedding_size),\n",
    "    in_features=random_embedding_size,\n",
    "    out_features=embedding_size,\n",
    "    embed_type=\"density\",\n",
    "    vsa=vsa_encoding\n",
    ")\n",
    "random_values_embeddings = hdencode(\n",
    "    torch.randn(no_values, random_embedding_size),\n",
    "    in_features=random_embedding_size,\n",
    "    out_features=embedding_size,\n",
    "    embed_type=\"density\",\n",
    "    vsa=vsa_encoding\n",
    ")\n",
    "kbe = torchhd.hash_table(random_keys_embeddings, random_values_embeddings)\n",
    "query = torchhd.inverse(random_keys_embeddings[0])\n",
    "result = torchhd.bind(kbe, query)\n",
    "torchhd.utils.plot_similarity(result, random_values_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d3936d",
   "metadata": {},
   "source": [
    "We can now run a test to compare a kbe entirely build from random hypervectors to a kbe constructed from real-world encoded signals. The parameters of the simulation are the same as before: 100 rounds, size of the embeddings equal to 1024 dimensions, and VSA format based on the BSC (Binary Spatter Codes) encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8d5b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_simulations = 100\n",
    "correct_validations = 0\n",
    "no_keys, no_values = 10, 10\n",
    "embedding_size = 1024\n",
    "random_embedding_size = 1024\n",
    "vsa_encoding = \"BSC\"\n",
    "\n",
    "for i in np.arange(no_simulations):\n",
    "    random_keys_embeddings = hdencode(\n",
    "        torch.randn(no_keys, random_embedding_size),\n",
    "        in_features=random_embedding_size,\n",
    "        out_features=embedding_size,\n",
    "        embed_type=\"density\",\n",
    "        vsa=vsa_encoding\n",
    "    )\n",
    "    random_values_embeddings = hdencode(\n",
    "        torch.randn(no_values, random_embedding_size),\n",
    "        in_features=random_embedding_size,\n",
    "        out_features=embedding_size,\n",
    "        embed_type=\"density\",\n",
    "        vsa=vsa_encoding\n",
    "    )\n",
    "    query_index = 0   # Our query will be random_keys_embeddings[0]\n",
    "    query_embedding = random_keys_embeddings[query_index]\n",
    "    correct_result = random_values_embeddings[query_index]\n",
    "    kbe = torchhd.hash_table(random_keys_embeddings, random_values_embeddings)\n",
    "    query = torchhd.inverse(query_embedding)\n",
    "    result = torchhd.bind(kbe, query)\n",
    "    nearest_match = torchhd.cleanup(result, random_values_embeddings)\n",
    "    if (nearest_match == correct_result).all():\n",
    "        correct_validations += 1\n",
    "print(f\"Query matching accuracy: {correct_validations / no_simulations * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e041ee68",
   "metadata": {},
   "source": [
    "The baseline accuracy of correct value retrieval using signals embeddings matches that of random samples. The density-based encoding works as expected, and produces accurate knowledge base embeddings. Up to this point, we have worked with relatively little dimensionality. Since we needed to construct kbes out of superpositions of 10 hypervectors, a size of 1024 dimensions sufficed. In case we want to construct more complex kbes (i.e. hundreds of superpositions), we need a much larger embedding size. As a rule of thumb, when using the BSC encoding, with $1000 \\times k$ dimensions we can superimpose $20 \\times k$ hypervectors keeping a lookup accuracy of around 99%. We will try now to encode our signals (as well as frequencies and radii labels) using hypervectors of 32768 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f88812",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_radii = dataset.radius.size\n",
    "no_frequencies = dataset.frequency.size\n",
    "embedding_size = 32768\n",
    "random_embedding_size = 1024\n",
    "vsa_encoding = \"BSC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9931d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_embeddings = hdencode(\n",
    "    torch.randn(no_frequencies, random_embedding_size),\n",
    "    in_features=random_embedding_size,\n",
    "    out_features=embedding_size,\n",
    "    embed_type=\"density\",\n",
    "    vsa=vsa_encoding)\n",
    "\n",
    "radius_embeddings = hdencode(\n",
    "    torch.randn(no_radii, random_embedding_size),\n",
    "    in_features=random_embedding_size,\n",
    "    out_features=embedding_size,\n",
    "    embed_type=\"density\",\n",
    "    vsa=vsa_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5d48b8",
   "metadata": {},
   "source": [
    "Since we have dramatically increased the size of our hypervectors, encoding will be much slower. The following method, **embed()**, will provide a nice way to see the encoding progress, and finally save our signals embeddings to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a90afe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(dataset, embedding_size, encoding, vsa_format):\n",
    "    # To iterate the signal dataset more comfortably, we have to permute radii with frequencies\n",
    "    tensor_dataset =  torch.tensor(dataset.values).permute(1, 0, 2)\n",
    "\n",
    "    # Using xArray unnamed function, the probability of memory overflow is extremely high.\n",
    "    # From both spatial and temporal standpoint, it is more efficient to iterate a torch\n",
    "    # tensor using standard loops. We will use tqdm to show the embedding phase progress.\n",
    "    frequencies = tensor_dataset.shape[0]\n",
    "    radii = tensor_dataset.shape[1]\n",
    "    signal_embeddings = []\n",
    "    total_iterations = frequencies * radii\n",
    "\n",
    "    with tqdm(total=total_iterations, desc=\"Embedding signals\", unit=\"pair\") as pbar:\n",
    "        for frequency in range(frequencies):\n",
    "            signal_embeddings_by_radius = []\n",
    "            for radius in range(radii):\n",
    "                signal = tensor_dataset[frequency, radius, :]\n",
    "                signal_embedding = hdencode(signal, dataset.time.size, embedding_size, encoding, vsa_format)\n",
    "                signal_embeddings_by_radius.append(signal_embedding)\n",
    "                # Progressbar settings: 1Hz refresh rate\n",
    "                pbar.update(1) # Update frequency\n",
    "                pbar.set_postfix(freq=frequency+1, radius=radius*10+10) \n",
    "            signal_embeddings.append(signal_embeddings_by_radius)\n",
    "    return signal_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e09f226",
   "metadata": {},
   "source": [
    "Now we can generate our density-based signals embeddings harnessing the method above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f22123",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_embeddings = embed(\n",
    "    dataset,\n",
    "    embedding_size=32768,\n",
    "    encoding=\"density\",\n",
    "    vsa_format=\"BSC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f162870",
   "metadata": {},
   "source": [
    "Additionally, we can store all the generated embeddings for signals, frequencies and radii into a single xarray data structure, in order to easily handle selections, slicing, and all the other typical operations on vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5b4f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_embeddings = xr.DataArray(\n",
    "    signal_embeddings,\n",
    "    dims=('frequency', 'radius', 'dimension'),\n",
    "    coords={\n",
    "        'frequency': np.arange(dataset.frequency.size) + 1,\n",
    "        'radius': np.arange(dataset.radius.size) * 10 + 10,\n",
    "        'dimension': np.arange(embedding_size) + 1\n",
    "    })\n",
    "\n",
    "frequency_embeddings = xr.DataArray(\n",
    "    frequency_embeddings,\n",
    "    dims=('frequency','dimension'),\n",
    "    coords={\n",
    "        'frequency': np.arange(np.shape(frequency_embeddings)[0]) + 1,\n",
    "        'dimension': np.arange(np.shape(frequency_embeddings)[1]) + 1\n",
    "    })\n",
    "\n",
    "radius_embeddings = xr.DataArray(\n",
    "    radius_embeddings,\n",
    "    dims=('radius','dimension'),\n",
    "    coords={\n",
    "        'radius': np.arange(np.shape(radius_embeddings)[0]) * 10 + 10,\n",
    "        'dimension': np.arange(np.shape(radius_embeddings)[1]) + 1\n",
    "    })\n",
    "\n",
    "embeddings = xr.merge([\n",
    "    signal_embeddings.to_dataset(name=\"signal_embeddings\"),\n",
    "    frequency_embeddings.to_dataset(name=\"frequency_embeddings\"),\n",
    "    radius_embeddings.to_dataset(name=\"radius_embeddings\")\n",
    "])\n",
    "\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e1f809",
   "metadata": {},
   "source": [
    "Finally, we will store our embeddings for subsequent reload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32e7fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.to_netcdf(os.path.join(output_folder, \"nrm.orx.embeddings.nc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ffedfe",
   "metadata": {},
   "source": [
    "Let's load our embeddings from disk to see if everything looks fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21adfe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = xr.load_dataset(os.path.join(input_folder, \"nrm.orx.embeddings.nc\"))\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c5f044",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings.signal_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5761ba74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings.frequency_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e60cb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings.radius_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8720ec6e",
   "metadata": {},
   "source": [
    "In order to retrive and use an embedding, we have to first convert it to a BSCTensor (or whatever torchhd format is preferred). The following is an example of such tensor casting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d1f390",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_signal_embedding = embeddings.signal_embeddings.sel(radius=300, frequency=1)\n",
    "print(test_signal_embedding)\n",
    "bsc_test_signal_embedding = torchhd.BSCTensor(test_signal_embedding.values)\n",
    "print(bsc_test_signal_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089eb4ac",
   "metadata": {},
   "source": [
    "Once we have all the embeddings in place, we can finally create the dictionaries to be used as classifiers. First, let's define some notation: \n",
    "- $\\rho; P \\rightarrow$ radius; set of all radii;\n",
    "- $\\phi; \\Phi \\rightarrow$ frequency; set of all frequencies;\n",
    "- $\\sigma_\\rho^\\phi \\rightarrow$ signal received for acoustic trap of radius $\\rho$ at central frequency $\\phi$\n",
    "- $\\oplus; \\otimes \\rightarrow$ superposition or coproduct or bundling; composition or product or binding.\n",
    "- $\\mathtt{kbe}[\\phi];\\ \\mathtt{kbe} \\rightarrow$ knowledge base for central frequency $\\phi$; knowledge base for all frequencies.\n",
    "\n",
    "We can produce two classifying solutions, mathematically defined as below:\n",
    "- a set of 10 dictionaries (one for each frequency $\\phi$), each superimposing 45 bindings: $$\\mathtt{kbe}[\\phi] = \\bigoplus\\limits_{\\rho \\in P} \\Bigl(\\rho \\otimes \\sigma_\\rho^\\phi\\Bigr)$$\n",
    "- a single dictionary, embedding 450 bindings: $$\\mathtt{kbe} = \\bigoplus\\limits_{\\substack{\\rho \\in P, \\\\ \\phi \\in \\Phi}} \\Bigl(\\phi \\otimes \\rho \\otimes \\sigma_\\rho^\\phi\\Bigr)$$\n",
    "\n",
    "Naturally, we have to provide to preserve both the frequency and the radius embeddings in order to execute the cleanup in subsequent steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9d1ef7",
   "metadata": {},
   "source": [
    "Let's create first $\\mathtt{kbe}[\\phi]$, for each central frequency $\\phi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b60abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "kbe_phi = []\n",
    "signal_embeddings = embeddings.signal_embeddings\n",
    "frequency_embeddings = embeddings.frequency_embeddings\n",
    "radius_embeddings = embeddings.radius_embeddings\n",
    "\n",
    "for i in np.arange(signal_embeddings.frequency.size):\n",
    "    signal_embeddings_by_radius = signal_embeddings.isel(frequency=i)\n",
    "    signal_as_key = torchhd.ensure_vsa_tensor(signal_embeddings_by_radius.values)\n",
    "    radius_as_value = torchhd.ensure_vsa_tensor(radius_embeddings.values)\n",
    "    kbe = torchhd.hash_table(signal_as_key, radius_as_value)\n",
    "    kbe_phi.append(kbe)\n",
    "\n",
    "kbe_phi = xr.DataArray(\n",
    "    kbe_phi,\n",
    "    dims=(\"frequency\", \"dimension\"),\n",
    "    coords={\n",
    "        \"frequency\": np.arange(frequency_embeddings.frequency.size) + 1,\n",
    "        \"dimension\": np.arange(frequency_embeddings.dimension.size) + 1\n",
    "    })\n",
    "\n",
    "print(kbe_phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f6cd55",
   "metadata": {},
   "source": [
    "Let's see if $\\mathtt{kbe}[\\phi]$ works as expected. We can randomly select a signal and, knowing only the central frequency, let's identify the acoustic trap radius."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hdsc (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
